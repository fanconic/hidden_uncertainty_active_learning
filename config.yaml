# Semester Project 2 at ETH
# Hidden Uncertainty for Active Learning
# For further questions contact Claudio Fanconi, fanconic@ethz.ch

name: "cityscape_gdgmm_r3"

random_state: [42]
save_plot: False
save_df: True
runs: 1
week: 0
wandb_logging: True

training:
    # active learning heuristic
    heuristic: "precomputed"
    reduction: "sum"
    shuffle_prop: 0.0

    # Neural Network Training
    batch_size: 16
    epochs: 60
    
    #Active learning steps
    ndata_to_label: [100, 200, 200, 200, 200, 1000, 2000] #, 2000, 2000, 2000, 10000, 10000, 10000, 10000, 0]
    iterations: 8
    initially_labelled: 100
    initially_balanced: False
    verbose: False

    #scheduler:
    scheduler: "poly"
    lr_reduce_factor: 0.1
    patience_lr_reduce: 20
    poly_reduce: 0.9

    # Early stopping
    early_stopping: True
    load_best_model: True
    patience_early_stopping: 20

    # For segmentation
    crop_size: 224

data:
    # Dataset parameters
    dataset: 'cityscapes_yu'
    nb_classes: 19
    img_rows: 256
    img_cols: 512
    img_channels: 3
    val_size: 0.2

    # Augmentation
    resize: True
    augmentation: True
    rgb_normalization: True
    mean:  [0.290101, 0.328081, 0.286964] # CIFAR10 [0.49139968, 0.48215827, 0.44653124]
    std: [0.182954, 0.186566, 0.184475] # CIFAR10 [0.24703233, 0.24348505, 0.26158768] 
    path: "/srv/beegfs02/scratch/density_estimation/data/fanconic/CityScapes" #only applies to CityScapes
    ignore_label: 255 # only for sematic segmenation

model:
    ensemble: 1 #set to 1 for no ensemble
    name: "MIR"
    mir_configs: # only applies if name == MIR
        backbone: "drn_d_22"
        reconstruction_weight: 1.0
        density_model: "gmm_seg"
        warmup: False
        feature_dims: 512
        normalize_features: True
        dim_reduction: -1
        decoder_bn: True
        num_res_blocks: 3

        # Greedy Search
        greedy_search: True
        search_step_size: 1
        reduction: "mean"
        metric: "val"

        # KNN
        knn_weights: "distance"
        knn_metric: "euclidean"
        knn_neighbours: 5
        max_samples: 1000

    # DeepLab
    deeplab_configs:
        backbone: "resnet18"
    
    # Data sizes - need to be the same as in "data"
    input_height: 256
    input_width: 512
    input_channels: 3
    output_size: 19

    # Layers for DNN and CNNS
    hidden_layers: [128, 128]
    kernel_sizes: [4, 4] # only applied if a (B)CNN is used
    dropout_probabilities: [0.05, 0.5] #[0.25, 0.5]
    use_bias: True

    # Monte Carlo Dropout parameters
    mc_dropout: False # used for MC dropout
    mc_iterations: 1 # iterations for MC dropout

    # BNN KL divergence loss
    kl_div_weight: 0.01 # only used for B(C)NN, to weight the kl_div loss

    #Pretrained
    pretrained_model: 
    pretrained: True
    use_torch_up: False

optimizer:
    type: "adam"
    lr: 0.001
    momentum: 0.9
    weight_decay: 0.0001
    betas: [0.9, 0.999]
    T0: 10