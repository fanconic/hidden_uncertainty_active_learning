# Semester Project 2 at ETH
# Hidden Uncertainty for Active Learning
# For further questions contact Claudio Fanconi, fanconic@ethz.ch

name: "cifar10_MIR_MCD_shuffle02"

random_state: [3, 11, 42]
save_plot: False
save_df: True
runs: 3
week: 5

training:
    heuristic: "entropy"
    shuffle_prop: 0.2
    batch_size: 64
    epochs: 60
    ndata_to_label: [100, 200, 200, 200, 200, 1000, 2000, 2000, 2000, 2000, 10000, 10000, 10000, 10000, 0]
    initially_labelled: 100
    initially_balanced: True
    verbose: False
    lr_reduce_factor: 0.1
    patience_lr_reduce: 8
    iterations: 15
    early_stopping: True
    load_best_model: True
    patience_early_stopping: 15

data:
    dataset: 'cifar10'
    nb_classes: 10
    augmentation: True
    img_rows: 32
    img_cols: 32
    img_channels: 3
    val_size: 0.2
    rgb_normalization: True

model:
    ensemble: 1 #set to 1 for no ensemble
    name: "resnet18"
    mir_configs: # only applies if name == MIR
        backbone: "resnet18"
        reconstruction_weight: 1.0
        density_model: "gmm"
        warmup: False
        feature_dims: 512
        normalize_features: True
        dim_reduction: -1
        decoder_bn: True
        num_res_blocks: 3

        # Greedy Search
        greedy_search: True
        search_step_size: 10

        # KNN
        knn_weights: "uniform"
        knn_metric: "cosine"
        knn_neighbours: 5

    input_height: 32
    input_width: 32
    input_channels: 3
    output_size: 10
    hidden_layers: [128, 128]
    kernel_sizes: [4, 4] # only applied if a (B)CNN is used
    dropout_probabilities: [0.05, 0.5] #[0.25, 0.5]
    use_bias: True
    mc_dropout: True # used for MC dropout
    mc_iterations: 10 # iterations for MC dropout
    kl_div_weight: 0.01 # only used for B(C)NN, to weight the kl_div loss
    pretrained: False

optimizer:
    lr: 0.001
    momentum: 0.9
    weight_decay: 0.0005
    betas: [0.9, 0.999]